import gradio as gr
import torch
import random
import numpy as np
import gc
import time
import csv
import os
import datetime
from PIL import Image

from comfy_extras.nodes_sd3 import EmptySD3LatentImage
from comfy_extras.nodes_custom_sampler import (
    BasicGuider, Noise_RandomNoise, KSamplerSelect, SamplerCustomAdvanced, BasicScheduler
)
from nodes import (
    UNETLoader, DualCLIPLoader, CLIPTextEncode, VAELoader, VAEDecode
)
from comfy_extras.nodes_flux import FluxGuidance
import requests
import threading

# =======================================
# 1) Í∏ÄÎ°úÎ≤å ÏÑ§Ï†ï
# =======================================
PROCESSING_TIME_PER_PIXEL = 0.00005

active_requests = 0
lock = threading.Lock()

LOG_FILE = "generation_logs.csv"

if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["timestamp", "is_korean", "prompt_length",
                         "width", "height", "steps", "seed", "guidance",
                         "estimated_time", "actual_time",
                         "original_prompt", "translated_prompt"])

def log_generation_data(is_korean, prompt, translated_prompt,
                        width, height, steps, seed, guidance, estimated_time, actual_time):
    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
    prompt_length = len(prompt)

    log_data = [
        timestamp,
        is_korean,
        prompt_length,
        width,
        height,
        steps,
        seed,
        guidance,
        estimated_time,
        actual_time,
        prompt,
        translated_prompt
    ]
    with open(LOG_FILE, mode="a", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(log_data)

def translate_korean_to_english(prompt):
    instruction = (
        "Translate the following Korean text into natural, concise English. "
        "The translation must be optimized for text-to-image generation. "
        "Do NOT add explanations, comments, or extra words‚Äîonly return the translated phrase.\n\n"
    ) 
    full_prompt = instruction + prompt
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "hf.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf:Q8_0",
            "prompt": full_prompt,
            "stream": False
        }
    )
    if response.status_code == 200:
        torch.cuda.empty_cache()
        gc.collect()
        return response.json()["response"].strip()
    else:
        return "Translation failed."

def random_seed():
    return random.randint(1, 10**15 - 1)

def estimate_processing_time(width, height, steps):
    width = width if width else 512
    height = height if height else 512
    steps = steps if steps else 25

    total_pixels = width * height
    steps_boost = (steps / 25)
    estimated_time = total_pixels * PROCESSING_TIME_PER_PIXEL * steps_boost
    return round(estimated_time, 2)

# =======================================
# 2) Ï†ÑÏó≠ Î™®Îç∏ (UNet, CLIP, VAE) Î°úÎìú
# =======================================
print("Loading models once at startup...")

# 2A. UNet Î°úÎìú
unet_loader = UNETLoader()
unet_name = "flux1-dev-fp8.safetensors"
unet_model = unet_loader.load_unet(unet_name, "default")[0]

# 2B. CLIP Î°úÎìú
clip_loader = DualCLIPLoader()
clip_model = clip_loader.load_clip("clip_l.safetensors", "t5xxl_fp16.safetensors", "flux")[0]
text_encoder = CLIPTextEncode()

# 2C. VAE Î°úÎìú
vae_loader = VAELoader()
vae_model = vae_loader.load_vae("ae.safetensors")[0]
vae_decoder = VAEDecode()

print("‚úÖ All models loaded successfully!")

# =======================================
# 3) Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± Ìï®Ïàò
# =======================================
def generate_image(prompt, width, height, seed, guidance, steps):
    global active_requests, unet_model, clip_model, vae_model

    with lock:
        active_requests += 1

    torch.cuda.empty_cache()
    start_time = time.time()

    # (1) ÌïúÍµ≠Ïñ¥Ïù∏ÏßÄ Í∞êÏßÄ
    is_korean = any("Í∞Ä" <= char <= "Ìû£" for char in prompt)

    # (2) Î≤àÏó≠
    translated_prompt = prompt
    if is_korean:
        translated_prompt = translate_korean_to_english(prompt)
        print(translated_prompt)

    # (3) ÏòàÏÉÅ Ï≤òÎ¶¨ ÏãúÍ∞Ñ
    estimated_time = estimate_processing_time(width, height, steps)

    # ==== Diffusion ÌååÏù¥ÌîÑÎùºÏù∏ ====    
    # Latent Image ÏÉùÏÑ±
    latent_generator = EmptySD3LatentImage()
    latent_image = latent_generator.generate(width, height, 1)[0]

    # Noise Ï∂îÍ∞Ä
    noise_generator = Noise_RandomNoise(seed)
    noisy_latent = noise_generator.generate_noise(latent_image)

    # ÏÉòÌîåÎü¨ ÏÑ†ÌÉù
    sampler_selector = KSamplerSelect()
    sampler_name = "euler"
    selected_sampler = sampler_selector.get_sampler(sampler_name)[0]

    # Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ï†ï
    scheduler = BasicScheduler()
    sigmas = scheduler.get_sigmas(unet_model, "simple", steps, 1.0)[0]

    # ÌÖçÏä§Ìä∏ Ïù∏ÏΩîÎî©
    encoded_text = text_encoder.encode(clip_model, translated_prompt)[0]

    # Flux Í∞ÄÏù¥ÎçòÏä§
    flux_guidance = FluxGuidance()
    conditioned_text = flux_guidance.append(encoded_text, guidance)[0]

    # Guider ÏÉùÏÑ±
    basic_guider = BasicGuider()
    guider = basic_guider.get_guider(unet_model, conditioned_text)[0]

    # ÏÉòÌîåÎßÅ Ïã§Ìñâ
    sampler_advanced = SamplerCustomAdvanced()
    sampled_latent, _ = sampler_advanced.sample(
        noise_generator, guider, selected_sampler, sigmas, latent_image
    )

    # VAE ÎîîÏΩîÎî©
    decoded_image = vae_decoder.decode(vae_model, sampled_latent)[0]
    decoded_image_np = (decoded_image[0].cpu().detach().numpy() * 255).astype(np.uint8)
    image_pil = Image.fromarray(decoded_image_np)

    with lock:
        active_requests -= 1

    total_time = round(time.time() - start_time, 2)

    # (4) Î°úÍπÖ (seed Ìè¨Ìï®)
    log_generation_data(
        is_korean, prompt, translated_prompt,
        width, height, steps, seed, guidance,
        estimated_time, total_time
    )

    return image_pil

# =======================================
# 4) ÎπÑÏú® Î≤ÑÌäº ÏΩúÎ∞±
# =======================================
def set_aspect_ratio(ratio):
    if ratio == "256:256":
        w, h = 256, 256
    elif ratio == "512:512":
        w, h = 512, 512
    elif ratio == "832:1216":
        w, h = 832, 1216
    elif ratio == "768:1344":
        w, h = 768, 1344
    else:
        w, h = 256, 256

    # steps=25Î°ú Í∞ÄÏ†ï
    steps_default = 25
    est_time = estimate_processing_time(w, h, steps_default)
    return (w, h, est_time)

# =======================================
# 5) Gradio Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
# =======================================
with gr.Blocks() as demo:
    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            # Flux.1 Dev
            Flux.1 DevÎäî **AI Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± Î™®Îç∏**Î°ú, ÏûÖÎ†•Ìïú ÌÖçÏä§Ìä∏(ÌîÑÎ°¨ÌîÑÌä∏)Ïóê Îî∞Îùº Ï∞ΩÏùòÏ†ÅÏù∏ Ïù¥ÎØ∏ÏßÄÎ•º ÏÉùÏÑ±ÌïòÎäî ÏµúÏã† AIÏûÖÎãàÎã§.  
            Stable DiffusionÏùò Î∞úÏ†ÑÎêú Î≤ÑÏ†ÑÏù¥Î©∞, **Îçî Îπ†Î•¥Í≥†, Îçî ÏÑ†Î™ÖÌïú Ïù¥ÎØ∏ÏßÄ**Î•º ÎßåÎì§Ïñ¥ÎÉÖÎãàÎã§.  

            ### üîó FluxÏóê ÎåÄÌï¥ Îçî ÏïåÍ≥† Ïã∂Îã§Î©¥?
            Flux.1 DevÏùò ÏõêÎ¶¨, Ïó∞Íµ¨ ÎÇ¥Ïö©, ÏµúÏã† ÏóÖÎç∞Ïù¥Ìä∏ Ï†ïÎ≥¥Îäî ÏïÑÎûò Í≥µÏãù Î∏îÎ°úÍ∑∏ÏóêÏÑú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.  
            [Flux Blog](https://blackforestlabs.ai/announcing-black-forest-labs/)
            """)

        with gr.Column():
            gr.Markdown("""
            ### **üîπ Guidance ScaleÏù¥ÎûÄ?**
            **Guidance Scale**ÏùÄ AIÍ∞Ä ÏûÖÎ†•Ìïú ÌÖçÏä§Ìä∏(ÌîÑÎ°¨ÌîÑÌä∏)Î•º **ÏñºÎßàÎÇò Ï§ëÏöîÌïòÍ≤å Ïó¨Í∏∏ÏßÄ** Í≤∞Ï†ïÌïòÎäî Îß§Í∞úÎ≥ÄÏàòÏûÖÎãàÎã§.  

            ‚úî **Í∞íÏù¥ ÎÇÆÏúºÎ©¥**  
            ‚Üí AIÍ∞Ä Îçî ÏûêÏú†Î°≠Í≤å Ï∞ΩÏûëÌïòÎ©∞, Ï∞ΩÏùòÏ†ÅÏù∏ Í≤∞Í≥ºÎ¨ºÏù¥ ÎÇòÏò¨ Ïàò ÏûàÏäµÎãàÎã§.  
            ‚úî **Í∞íÏù¥ ÎÜíÏúºÎ©¥**  
            ‚Üí ÏûÖÎ†•Ìïú Î¨∏Ïû•(ÌîÑÎ°¨ÌîÑÌä∏)Í≥º ÏµúÎåÄÌïú ÏùºÏπòÌïòÎèÑÎ°ù Ïù¥ÎØ∏ÏßÄÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.  

            üí° **Tip:**  
            - üé® Ï∞ΩÏùòÏ†ÅÏù∏ Í∑∏Î¶ºÏù¥ ÌïÑÏöîÌïòÎã§Î©¥ ÎÇÆÏùÄ Í∞í(Ïòà: 2~4)  
            - üì∏ ÏûÖÎ†• Î¨∏Ïû•Í≥º ÏµúÎåÄÌïú ÏùºÏπòÌïòÎäî Ïù¥ÎØ∏ÏßÄÍ∞Ä ÌïÑÏöîÌïòÎ©¥ ÎÜíÏùÄ Í∞í(Ïòà: 8~10)  
            """)

    with gr.Row():
        with gr.Column():
            prompt = gr.Textbox(label="Prompt", value="A futuristic city with neon lights")
            width = gr.Slider(0, 1600, value=256, step=32, label="Width")
            height = gr.Slider(0, 1600, value=256, step=32, label="Height")

            with gr.Row():
                aspect_ratio_s1_1 = gr.Button("256:256")
                aspect_ratio_1_1 = gr.Button("512:512")
                aspect_ratio_16_9 = gr.Button("832:1216")
                aspect_ratio_4_3 = gr.Button("768:1344")

            seed = gr.Number(value=random_seed(), label="Seed")
            seed_button = gr.Button("Randomize Seed")
            guidance = gr.Slider(1.0, 10.0, value=3.5, step=0.1, label="Guidance Scale")
            steps = gr.Slider(1, 100, value=25, step=1, label="Sampling Steps")

            estimated_time = gr.Textbox(label="Estimated Wait Time (seconds)", interactive=False)

            generate_button = gr.Button("Generate Image")

        with gr.Column():
            output_image = gr.Image(label="Generated Image")

    # Ïä¨ÎùºÏù¥Îçî Î≥ÄÍ≤Ω => ÏòàÏÉÅ ÎåÄÍ∏∞ ÏãúÍ∞Ñ ÏóÖÎç∞Ïù¥Ìä∏
    def update_estimated_time(w, h, s):
        return estimate_processing_time(w, h, s)

    width.change(update_estimated_time, inputs=[width, height, steps], outputs=[estimated_time], queue=False)
    height.change(update_estimated_time, inputs=[width, height, steps], outputs=[estimated_time], queue=False)
    steps.change(update_estimated_time, inputs=[width, height, steps], outputs=[estimated_time], queue=False)

    # ÎπÑÏú® Î≤ÑÌäº => Width, Height, EstimatedTime
    aspect_ratio_s1_1.click(fn=lambda: set_aspect_ratio("256:256"), outputs=[width, height, estimated_time], queue=False)
    aspect_ratio_1_1.click(fn=lambda: set_aspect_ratio("512:512"),   outputs=[width, height, estimated_time], queue=False)
    aspect_ratio_16_9.click(fn=lambda: set_aspect_ratio("832:1216"), outputs=[width, height, estimated_time], queue=False)
    aspect_ratio_4_3.click(fn=lambda: set_aspect_ratio("768:1344"),   outputs=[width, height, estimated_time], queue=False)

    seed_button.click(fn=random_seed, outputs=[seed])

    def wrapper_generate_image(p, w, h, sd, g, st):
        return generate_image(p, w, h, sd, g, st)

    generate_button.click(
        fn=wrapper_generate_image,
        inputs=[prompt, width, height, seed, guidance, steps],
        outputs=[output_image],
        queue=True,
        concurrency_limit=1
    )

if __name__ == "__main__":
    demo.launch()
